# LangGraph Text Analysis Workflow

A comprehensive LangGraph project demonstrating multi-node workflows, state management, and Ollama integration.

## Quick Start

1. **Install Ollama** (if not already installed):
   - Visit https://ollama.ai
   - Download and install for your OS

2. **Start Ollama and pull a model**:
   ```bash
   ollama serve
   ollama pull llama3.2
   ```

3. **Install Python dependencies**:
   ```bash
   cd backend
   pip install -r requirements.txt
   ```

4. **Run the demo**:
   ```bash
   python main.py
   ```

## Project Features

### Architecture
- **2 Processing Nodes**:
  - `input_processor`: Analyzes text and counts words
  - `summarizer`: Generates summary and sentiment analysis
  
- **State Management**: TypedDict-based state with 4 fields:
  - `input_text`: Original user input
  - `word_count`: Calculated by first node
  - `summary`: Generated by second node
  - `sentiment`: Generated by second node

### Key Features
- ✅ Configurable Ollama model selection
- ✅ Memory persistence with checkpointing
- ✅ Streaming support for real-time updates
- ✅ Multiple output formats (text, JSON, markdown)
- ✅ Comprehensive logging
- ✅ Error handling
- ✅ CLI interface with arguments
- ✅ Example scripts

## Usage Examples

### Basic Usage
```python
from src.graph.workflow import run_workflow

result = run_workflow("Your text here...")
print(result["summary"])
print(result["sentiment"])
```

### Custom Model
```python
result = run_workflow(
    input_text="Your text...",
    model_name="llama3.2"
)
```

### Streaming
```python
from src.graph.workflow import stream_workflow

for update in stream_workflow("Your text..."):
    print(update)
```

### CLI Usage
```bash
# Use sample text
python main.py --sample 0

# Custom text
python main.py --text "Your custom text here"

# Different model
python main.py --sample 0 --model llama3.2

# JSON output
python main.py --sample 1 --format json

# Streaming mode
python main.py --sample 0 --stream
```

## Project Structure

```
backend/
├── src/
│   ├── config/
│   │   └── models.py          # Model configuration & initialization
│   ├── graph/
│   │   ├── state.py            # State schema (TypedDict)
│   │   ├── nodes.py            # Node implementations
│   │   └── workflow.py         # Graph construction & compilation
│   └── utils/
│       └── helpers.py          # Utility functions
├── examples/
│   └── examples.py             # Usage examples
├── main.py                     # CLI entry point
├── requirements.txt
└── README.md
```

## Advanced Features

### Model Presets
```python
from src.config.models import get_model_from_preset

# Use predefined configurations
model = get_model_from_preset("creative")    # High temperature
model = get_model_from_preset("precise")     # Low temperature
model = get_model_from_preset("balanced")    # Default
```

### Persistent Conversations
```python
workflow = create_workflow()

# First conversation
result1 = workflow.invoke(
    {"input_text": "First text..."},
    {"configurable": {"thread_id": "user-123"}}
)

# Continue same conversation
result2 = workflow.invoke(
    {"input_text": "Second text..."},
    {"configurable": {"thread_id": "user-123"}}
)
```

### Direct Node Usage
```python
from src.graph.nodes import input_processor, summarizer

state = {"input_text": "Your text..."}
state.update(input_processor(state))
state.update(summarizer(state))
```

## Use Case

This workflow implements a text analysis pipeline:
1. **Input Stage**: User provides text
2. **Processing Stage**: Calculate word count and metadata
3. **Analysis Stage**: Generate summary and sentiment
4. **Output Stage**: Return comprehensive results

Perfect for:
- Content analysis
- Document summarization
- Sentiment monitoring
- Text preprocessing pipelines

## Configuration

### Environment Variables
Create a `.env` file:
```bash
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=llama3.2
```

### Model Configuration
Edit `src/config/models.py` to customize:
- Default model name
- Temperature settings
- Context window size
- Sampling parameters

## Requirements

- Python 3.11+
- Ollama running locally
- At least one Ollama model installed

## Troubleshooting

**Model not found**:
```bash
ollama pull llama3.2
```

**Connection refused**:
```bash
ollama serve
```

**Import errors**:
```bash
pip install -r requirements.txt
```

## Examples

Run the examples script to see all features:
```bash
cd examples
python examples.py
```

## Future Extensions

This project is designed to be extensible. Consider adding:
- Additional analysis nodes (keyword extraction, entity recognition)
- Database persistence
- API endpoints
- Frontend interface
- Batch processing
- Advanced error recovery
- Custom reducers for state management

## License

MIT License - feel free to use and modify for your needs.
