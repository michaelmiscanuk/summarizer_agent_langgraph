# ğŸ¯ Text Analysis Application - Full Stack

A modern, full-stack AI-powered text analysis application with beautiful UI and intelligent backend. Analyze any text to get instant summaries and sentiment analysis using LangGraph workflows and LLM models.

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)
![Flask](https://img.shields.io/badge/Flask-3.0+-lightgrey.svg)
![LangGraph](https://img.shields.io/badge/LangGraph-0.2+-orange.svg)

## âœ¨ Features

### Frontend
- ğŸ¨ **Beautiful Modern UI** - Gradient backgrounds, smooth animations, responsive design
- âš¡ **Fast & Intuitive** - Real-time character counter, instant feedback
- ğŸ“± **Mobile Friendly** - Works perfectly on all devices
- ğŸ¯ **User-Focused** - Sample texts, helpful error messages, loading states
- ğŸŒ™ **Dark Theme** - Easy on the eyes

### Backend
- ğŸ¤– **LangGraph Workflows** - Multi-node processing pipeline
- ğŸ§  **Multiple AI Models** - Choose from various Ollama models
- ğŸ“Š **Comprehensive Analysis** - Word count, summary, sentiment
- ğŸ”Œ **REST API** - Clean, documented FastAPI endpoints
- ğŸ”’ **Secure** - CORS enabled, input validation

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      USER BROWSER                        â”‚
â”‚                  (Beautiful Modern UI)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â”‚ HTTP/JSON
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FRONTEND (Vercel)                       â”‚
â”‚                                                          â”‚
â”‚  â€¢ Flask Web Server                                     â”‚
â”‚  â€¢ Modern CSS3 + Vanilla JS                            â”‚
â”‚  â€¢ API Proxy to Backend                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â”‚ REST API
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 BACKEND (Render.com)                     â”‚
â”‚                                                          â”‚
â”‚  â€¢ FastAPI REST API                                     â”‚
â”‚  â€¢ LangGraph Workflow:                                  â”‚
â”‚    â”œâ”€ Node 1: Input Processing                         â”‚
â”‚    â””â”€ Node 2: LLM Analysis                             â”‚
â”‚  â€¢ Ollama Integration                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
langgraph_test1/
â”œâ”€â”€ backend/                    # FastAPI Backend
â”‚   â”œâ”€â”€ api.py                 # FastAPI app with endpoints
â”‚   â”œâ”€â”€ main.py                # Original CLI app
â”‚   â”œâ”€â”€ requirements.txt       # Backend dependencies
â”‚   â”œâ”€â”€ render.yaml           # Render deployment config
â”‚   â”œâ”€â”€ DEPLOYMENT.md         # Backend deployment guide
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ config/           # Model configuration
â”‚       â”œâ”€â”€ graph/            # LangGraph workflow
â”‚       â”‚   â”œâ”€â”€ nodes.py     # Processing nodes
â”‚       â”‚   â”œâ”€â”€ state.py     # State management
â”‚       â”‚   â””â”€â”€ workflow.py  # Workflow definition
â”‚       â””â”€â”€ utils/           # Helper functions
â”‚
â””â”€â”€ frontend/                  # Flask Frontend
    â”œâ”€â”€ app.py                # Flask application
    â”œâ”€â”€ requirements.txt      # Frontend dependencies
    â”œâ”€â”€ vercel.json          # Vercel deployment config
    â”œâ”€â”€ README.md            # Frontend documentation
    â”œâ”€â”€ static/
    â”‚   â”œâ”€â”€ css/
    â”‚   â”‚   â””â”€â”€ style.css    # Beautiful global styles
    â”‚   â””â”€â”€ js/
    â”‚       â””â”€â”€ main.js      # Frontend JavaScript
    â””â”€â”€ templates/
        â”œâ”€â”€ index.html       # Main page
        â”œâ”€â”€ about.html       # About page
        â”œâ”€â”€ 404.html        # 404 error page
        â””â”€â”€ 500.html        # 500 error page
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11 or higher
- Ollama installed and running (for local development)
- Git
- Node.js (for Vercel CLI, optional)

### Local Development

#### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd langgraph_test1
```

#### 2. Start Backend

```bash
cd backend

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # macOS/Linux

# Install dependencies
pip install -r requirements.txt

# Start Ollama (separate terminal)
ollama serve

# Pull a model
ollama pull qwen2.5-coder:0.5b

# Start backend
python api.py
```

Backend will run at `http://localhost:8000`

#### 3. Start Frontend

```bash
cd frontend

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # macOS/Linux

# Install dependencies
pip install -r requirements.txt

# Set up environment
copy .env.example .env
# Edit .env and set API_BASE_URL=http://localhost:8000

# Start frontend
python app.py
```

Frontend will run at `http://localhost:5000`

#### 4. Test the Application

1. Open browser: `http://localhost:5000`
2. Enter some text
3. Click "Analyze Text"
4. View results!

## ğŸŒ Deployment

### Backend Deployment (Render.com)

1. **Create Render Account**: [render.com](https://render.com)

2. **Deploy Backend**:
   - Create new Web Service
   - Connect GitHub repository
   - Set root directory: `backend`
   - Build: `pip install -r requirements.txt`
   - Start: `uvicorn api:app --host 0.0.0.0 --port $PORT`

3. **Configure Environment Variables**:
   ```
   PYTHON_VERSION=3.11.0
   OLLAMA_BASE_URL=<your-ollama-server>
   ```

4. **Get Backend URL**: `https://your-app.onrender.com`

ğŸ“– **Detailed Guide**: See `backend/DEPLOYMENT.md`

### Frontend Deployment (Vercel)

1. **Install Vercel CLI** (optional):
   ```bash
   npm install -g vercel
   ```

2. **Deploy**:
   ```bash
   cd frontend
   vercel
   ```

3. **Set Environment Variables** in Vercel Dashboard:
   ```
   API_BASE_URL=https://your-backend.onrender.com
   SECRET_KEY=<random-secure-string>
   FLASK_ENV=production
   ```

4. **Update CORS** in `backend/api.py`:
   ```python
   allow_origins=[
       "https://your-frontend.vercel.app",
   ]
   ```

ğŸ“– **Detailed Guide**: See `frontend/README.md`

## ğŸ§ª Testing

### Test Backend API

```bash
# Health check
curl http://localhost:8000/health

# Analyze text
curl -X POST http://localhost:8000/api/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "text": "This is amazing! I love it.",
    "model_name": "qwen2.5-coder:0.5b"
  }'

# View API docs
# Open: http://localhost:8000/docs
```

### Test Frontend

1. Open `http://localhost:5000`
2. Try sample texts
3. Test with long text (>10,000 characters should show error)
4. Test different models
5. Check responsive design on mobile

## ğŸ“Š API Endpoints

### Backend (FastAPI)

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | API info |
| `/health` | GET | Health check |
| `/api/analyze` | POST | Analyze text |
| `/api/models` | GET | List available models |
| `/docs` | GET | Swagger UI |

### Frontend (Flask)

| Route | Description |
|-------|-------------|
| `/` | Main analysis page |
| `/about` | About page |
| `/health` | Health check |
| `/api/analyze` | Proxy to backend |
| `/api/models` | Proxy to backend |

## ğŸ”§ Configuration

### Backend Environment Variables

```env
PYTHON_VERSION=3.11.0
OLLAMA_BASE_URL=http://localhost:11434
PORT=8000
```

### Frontend Environment Variables

```env
API_BASE_URL=http://localhost:8000
SECRET_KEY=your-secret-key
FLASK_ENV=development
```

## ğŸ¨ Customization

### Modify Styles

Edit `frontend/static/css/style.css`:

```css
:root {
    --primary-color: #6366f1;  /* Change primary color */
    --secondary-color: #ec4899; /* Change secondary color */
    /* ... more variables ... */
}
```

### Add New Models

Edit `backend/src/config/models.py`:

```python
AVAILABLE_MODELS = {
    "your-model": {
        "name": "your-model",
        "temperature": 0.7,
    }
}
```

### Modify Workflow

Edit `backend/src/graph/nodes.py` and `backend/src/graph/workflow.py` to add new processing nodes or change the workflow logic.

## ğŸ“ˆ Performance

- **Backend**: FastAPI is one of the fastest Python frameworks
- **Frontend**: Lightweight Flask with minimal dependencies
- **UI**: No framework bloat, pure CSS and vanilla JS
- **Cold Start**: ~30-60 seconds on free tier (Render)
- **Response Time**: ~2-5 seconds for analysis (depends on model)

## ğŸ›¡ï¸ Security

- âœ… CORS properly configured
- âœ… Input validation on both frontend and backend
- âœ… Environment variables for sensitive data
- âœ… No data persistence (privacy-first)
- âœ… HTTPS on both Vercel and Render
- âœ… Rate limiting (can be added)

## ğŸ› Troubleshooting

### "Cannot connect to backend"

1. Check if backend is running
2. Verify `API_BASE_URL` in frontend `.env`
3. Check CORS settings
4. Review backend logs

### "Analysis taking too long"

1. Try smaller text
2. Use faster model (qwen2.5-coder:0.5b)
3. Check Ollama is running
4. Verify server resources

### Deployment Issues

1. Check deployment logs
2. Verify environment variables
3. Ensure Python version compatibility
4. Review requirements.txt

## ğŸ“š Documentation

- [Backend Deployment Guide](backend/DEPLOYMENT.md)
- [Frontend Documentation](frontend/README.md)
- [Backend Architecture](backend/ARCHITECTURE.md)
- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [LangGraph Docs](https://langchain-ai.github.io/langgraph/)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- **LangGraph** - For workflow orchestration
- **FastAPI** - For modern API framework
- **Flask** - For simple frontend framework
- **Ollama** - For local LLM inference
- **Vercel** - For frontend hosting
- **Render** - For backend hosting

## ğŸ“ Support

For issues, questions, or suggestions:
- Open an issue on GitHub
- Check the troubleshooting sections
- Review documentation

---

**Built with â¤ï¸ using Python, FastAPI, Flask, LangGraph, and Modern CSS**

Happy analyzing! ğŸ‰
